{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States\n",
    "States is a list of all possible states in given environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "states = ['Class 1', 'Class 2', 'Class 3', 'Facebook', 'Stop']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Action\n",
    "Action class represents an singe action that agent can take.\n",
    "* to - represents next state when agent will take this action\n",
    "* p - probability of this action. Default it is 1, but if single Action can lead to multiple states p is probability that we end up in to state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action:\n",
    "    def __init__(self, to, p=1):\n",
    "        self.to = to\n",
    "        self.p = p\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"(to: {}, p: {})\"\\\n",
    "            .format(self.to, self.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State actions\n",
    "State actions is an dictionary where key is a state name and a value is all possible action that can be taken from this state.\n",
    "Actions for state are also represented as dictionaries. Key is a action name and value is a list of Action classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_actions = {\n",
    "    'Class 1': {\n",
    "        'Study': [Action('Class 2')],\n",
    "        'Facebook': [Action('Facebook')]\n",
    "    },\n",
    "    'Class 2': {\n",
    "        'Study': [Action('Class 3')],\n",
    "        'Sleep': [Action('Stop')]\n",
    "    },\n",
    "    'Class 3': {\n",
    "        'Study': [Action('Stop')],\n",
    "        'Pub': [\n",
    "            Action('Class 3', 0.4),\n",
    "            Action('Class 2', 0.4),\n",
    "            Action('Class 1', 0.2)\n",
    "        ]\n",
    "    },\n",
    "    'Facebook': {\n",
    "        'Facebook': [Action('Facebook')],\n",
    "        'Quit': [Action('Class 1')]\n",
    "    },\n",
    "    'Stop': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State action reward\n",
    "This is dictionary that represends a reward for taking an action A when we are in state S.\n",
    "Key is state name and value is a dictionary with pairs: action name -> reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_actions_reward = {\n",
    "    'Class 1': {\n",
    "        'Study': -2,\n",
    "        'Facebook': -1\n",
    "    },\n",
    "    'Class 2': {\n",
    "        'Study': -2,\n",
    "        'Sleep': 0\n",
    "    },\n",
    "    'Class 3': {\n",
    "        'Study': 10,\n",
    "        'Pub': 1\n",
    "    },\n",
    "    'Facebook': {\n",
    "        'Facebook': -1,\n",
    "        'Quit': 0\n",
    "    },\n",
    "    'Stop': {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminal states\n",
    "List of states that ends an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_states = ['Stop']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value function and Policy\n",
    "**Value function V** is dictionary that for each state S stores value of this state. That is, states with higer value are simply better.\n",
    "\n",
    "**Policy** is another dictionary that for state S stores an action A that agent should take in this state.\n",
    "\n",
    "Our goal is to find **optimal policy** - policy that will maximalize reward.\n",
    "\n",
    "**Discount** is value from 0 to 1 inclusive. Each next step is discounted with **discount** factor. If closer to 0 agent will prefer imidiete rewards and will not care about rewards in far future. When closer to 1 agent will prefer to take actions that will grant him hiher rewards in the futrue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = dict((s, 0) for s in states)\n",
    "Policy = dict((s, None) for s in states)\n",
    "discount = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimum class\n",
    "Optimum class represents optimal value that maximaze **state value** for given state.\n",
    "* **value** is value of given state\n",
    "* **action** is action that should be taken in given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimum:\n",
    "    def __init__(self, value, action):\n",
    "        self.value = value\n",
    "        self.action = action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v(state) function\n",
    "**v(state) function** findes Optimum for given state.\n",
    "It is calculated as follows:\n",
    "1. For each possible action a for given state s calculate:\n",
    "  V(s) = r + d * SUM(p * V(ns)) where:\n",
    "  * V(s) is value of state s\n",
    "  * r is reward for taking action a\n",
    "  * SUM is sum for all possible output states of taking action a\n",
    "  * p is probability of going to state ns after taking action a\n",
    "  * ns is state that result of taking action a\n",
    "  * V(ns) is value of state ns\n",
    "2. From above calculation take maximum and this is new value for state s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal value function for state\n",
    "def v(state):\n",
    "    values = []\n",
    "    for action in state_actions[state].keys():\n",
    "        reward = state_actions_reward[state][action]\n",
    "        future = discount * sum([act.p * V[act.to] for act in state_actions[state][action]])\n",
    "        values.append(Optimum(reward + future, action))\n",
    "        \n",
    "    optimum = max(values, key=lambda item: item.value)\n",
    "    return optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value interation\n",
    "This is method of calculating Bellman Equation using dynamic programming paradigm.\n",
    "It takes following parameters:\n",
    "* n - number of iterations\n",
    "* theta - threshold of convergence\n",
    "* verobse - display iterations\n",
    "Method calculate optimal state value and policy for each state. It stops after:\n",
    "* Iterating n times, or\n",
    "* Achieving convergence of state-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(n=10, theta=0.02, verbose=False):\n",
    "    for _ in range(n):\n",
    "        old_V = np.array([x for x in V.values()])\n",
    "        for state in states:\n",
    "            if state in terminal_states:\n",
    "                continue\n",
    "            opt = v(state)\n",
    "            Policy[state] = opt.action\n",
    "            V[state] = opt.value\n",
    "        new_V = np.array([x for x in V.values()])\n",
    "        delta = abs(max(old_V-new_V))\n",
    "        if verbose: \n",
    "            print(\"Value_function: V={}\".format(V))\n",
    "        if delta < theta:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run and display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Class 1': 6, 'Class 2': 8, 'Class 3': 10, 'Facebook': 6, 'Stop': 0}\n",
      "{'Class 1': 'Study', 'Class 2': 'Study', 'Class 3': 'Study', 'Facebook': 'Quit', 'Stop': None}\n"
     ]
    }
   ],
   "source": [
    "discount = 1\n",
    "value_iteration()\n",
    "print(V)\n",
    "print(Policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
